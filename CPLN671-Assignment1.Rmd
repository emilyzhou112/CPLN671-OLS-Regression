---
title: "Using OLS Regression to Predict Median House Values in Philadelphia"
author: "Emily, Ziyi, Emma"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: simplex
    toc: yes
    toc_float: yes
    code_folding: hide
    code_download: yes
bibliography: references.bib
csl: apa.csl 

editor_options:
  markdown:
    wrap: sentence
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

```


```{r libraries, message=FALSE, warning=FALSE, include=FALSE}

options(scipen = 999)

# all the library loaded
library(tidyverse)
library(here)
library(kableExtra)
library(ggplot2)
library(ggcorrplot)
library(sf)
library(patchwork)
library(MASS)
library(caret)

```


```{r read data, message=FALSE, warning=FALSE, include=FALSE}

# read in data
data <- read_csv(here("data", "RegressionData.csv"))

```


# Introduction

Housing values in Philadelphia have seen a consistent increase in recent years, making it an important area of concern and research for urban planners and policymakers. This upward trend has raised issues around housing affordability, particularly for low and middle-income households. Many neighborhoods have experienced gentrification, leading to the displacement of long-term residents and changing the socio-economic landscape of the city. Examining housing values is critical for understanding these shifts and addressing concerns related to affordability, neighborhood stability, and equitable development across Philadelphia.

To better understand the potential factors influencing housing values in Philadelphia, we identified several key variables, including educational attainment, vacancy rates, the proportion of detached single-family homes, and poverty levels. These factors are closely linked to housing market trends and provide insights into the socio-economic conditions that may affect property values across the city.

Educational attainment is closely tied to economic outcomes and this includes property values. Studies suggest that areas with higher levels of education tend to have more robust local economies, higher incomes, and increased demand for housing, which pushes up home values @Scott2012. High vacancy rates are typically associated with declining neighborhoods and lower median house values. Vacant properties can be signs of economic distress, contributing to neighborhood decline through reduced upkeep, higher crime rates, and a negative perception of the area @mallach2018empty. In Philadelphia, the issue of vacancy has been particularly problematic in certain neighborhoods, where a large number of abandoned properties has led to significant reductions in housing values @kromer2002vacant.

The proportion of single-family homes in an area can also influence housing values, depending on the local market. Single-family homes are generally more desirable in many U.S. housing markets, as they offer more space, privacy, and perceived stability compared to multi-family units @glaeser2018economic. Research also consistently demonstrates a strong correlation between poverty levels and housing values. Areas with higher poverty rates often experience reduced demand for housing, as well as diminished investment in local infrastructure and services, which in turn can lead to lower property values @Galster2008. 

# Methods

## Data Cleaning

The original dataset used in this study consisted of 1816 census block groups in Philadelphia according to the 2000 Census, but to ensure the dataset was suitable for regression analysis, several data cleaning procedures were performed. First, every block group in the dataset with a population of less than 40 was removed. These block groups represent sparsely populated or unusually housed areas that may introduce noise into the analysis. Secondly, block groups without any housing units were also excluded due to their lack of significance in a study centered on housing attributes. Thirdly, block groups with a median housing value of less than \$10,000 were not included. These block groups do not accurately reflect the reality of the typical housing market, and the unusually low home values in these block groups may bias the results of the study. Finally, a unique combination of values - median property values over \$800,000 and typical household incomes less than \$8,000 - led to the exclusion of one block group in North Philadelphia. As a clear outlier, it was excluded to avoid biasing the results. The final dataset includes 1,720 observations, each corresponding to a different Philadelphia block group, and has the following attributes:

- **POLY_ID**: Census block group ID
- **MEDHVAL**: Median value of all owner occupied housing units
- **PCTBACHMOR**: Proportion of residents in Block Group with at least a Bachelorâ€™s degree
- **PCTVACANT**: Proportion of housing units that are vacant
- **PCTSINGLES**: Percent of housing units that are detached single family houses 
- **NBELPOV100**: Number of households with incomes below 100% poverty level (i.e., number of households living in poverty)
- **MEDHHINC**: Median household income


## Exploratory Data Analysis

We begin by examining the summary statistics and distributions of the dependent and independent variables. This will provide a better understanding of the data and help identify any potential outliers or unusual patterns that may need to be addressed before proceeding with regression analysis. In our case, the median home value (MEDHVAL) is the dependent variable. The independent variables are the proportion of the population with a bachelor's degree or higher (PCBACHMORE), the number of households in poverty (NBELPOV100), the proportion of vacant homes (PCTVACANT), and the proportion of single family homes (PCTSINGLES). We computed the mean and standard deviation for each variable to understand the central tendency and variability of the data. If results show that the variable is non-normally distributed, we apply a log transformation to normalize the data.

This is followed by investigating the correlations between the predictor variables to avoid multicollinearity. Correlation is a statistical measure that describes the strength and direction of a relationship between variables. The correlation coefficient \(r\) quantifies the degree to which two variables are linearly related. The formula for the sample correlation coefficient \(r\) is as follows: 

$$
r = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n} (x_i - \bar{x})^2 \sum_{i=1}^{n} (y_i - \bar{y})^2}}
$$
In a more concise way, this above formula is also equivalent to the following: 

$$
r = \frac{1}{n-1} \sum_{i=1}^{n} \left( \frac{x_i - \bar{x}}{S_x} \right) \left( \frac{y_i - \bar{y}}{S_y} \right)
$$
where \(x_i\) and \(y_i\) are individual data points, \(\bar{x}\) and \(\bar{y}\) are the means of the variables \(x\) and \(y\), \(S_x\) and \(S_y\) are the standard deviations of the variables \(x\) and \(y\) respectively. 

For each observation \(i\), the formula calculates the difference between the data point \(x_i\), \(y_i\) and the mean of the variables and then divides them by their standard deviation. Then, these standardized values are multiplied together to assess the extent to which the two variables deviate from their respective means in the same direction. The result of this multiplication is summed across all observations, which aggregates the individual contributions to the correlation. Finally, dividing this sum by \(n-1\) where \(n\) is the number of observations yields the sample correlation coefficient \(r\).

The correlation coefficient **\(r\)** can range from -1 to +1. A value of +1 indicates a perfect positive linear relationship, where increases in one variable correspond with increases in the other, while a value of -1 indicates a perfect negative linear relationship. A value of 0 suggests no linear relationship between the variables, meaning changes in one do not predict changes in the other. Understanding the correlations between the predictors is critical to identify whether any variables are highly correlated, which would suggest multicollinearity. Multicollinearity can distort the regression results by inflating the standard errors of the coefficients, which lead to less reliable interpretations. 

## Multiple Regression Analysis

After exploring the data, we conduct a multiple regression analysis to examine the relationship between the dependent variable (median house value) and the independent variables (educational attainment, poverty levels, vacancy rates, and single-family housing). Regression analysis is a statistical method used to examine the relationship between a dependent variable and one or more independent variables. It allows researchers to understand how the dependent variable changes as the independent variables vary and make inferences about the relationships among the variables. The model estimates coefficients for each independent variable, which quantify the expected change in the dependent variable for a one-unit increase in the independent variable, holding all other variables constant. For our problem, the equation for the multiple regression model is as follows:


$$
\text{LNMEDHVAL} = \beta_0 + \beta_1 \text{PCTVACANT} + \beta_2 \text{PCTSINGLES} + \beta_3 \text{PCTBACHMOR} + \beta_4 \text{LN(LNNBELPOV100)} + \epsilon
$$

where \(\text{LNMEDHVAL}\) is the log-transformed median house value, \(\text{PCTVACANT}\) is the proportion of vacant housing units, \(\text{PCTSINGLES}\) is the proportion of single-family detached homes, \(\text{PCTBACHMOR}\) is the percentage of residents with a bachelor's degree or higher, \(\text{LN(LNNBELPOV100)}\) is the log-transformed number of households living below the poverty line. 


\(\beta_0\) is the intercept, \(\beta_1, \beta_2, \beta_3, \beta_4\) are the coefficients for each independent variable, and \(\epsilon\) is the residual error term. The coefficients \(\beta_1, \beta_2, \beta_3, \beta_4\) represent the expected change in \(\text{LNMEDHVAL}\) for a one-unit increase in the corresponding independent variable, holding all other variables constant. The error term \(\epsilon\) accounts for the variability in \(\text{LNMEDHVAL}\) that is not explained by the independent variables.

There are several assumptions associated with regression analysis that needs to be check before we further proceed. First, **linearity** asserts that there should be a linear relationship between the dependent variables and independent variable. To verify this assumption, we made scatterplots to visualize the relationship between the relationship between **\(y\)** and each of the predictor. 

Second the assumption of **independence of observations** requires that the observations within the dataset be independent of one another, without any spatial, temporal, or other dependencies. In spatial contexts, we made choropleth maps to assess the presence of spatial autocorrelation in the residuals or dependent variable. 

Third, the **normality of residuals** assumption states that the residuals should follow a normal distribution. While this assumption is less critical in large samples due to the Central Limit Theorem, it remains desirable for inference purposes. To evaluate this assumption, we made a histogram of the residuals - a bell-shaped curve would suggest that the residuals are normally distributed.

Another important assumption is **homoscedasticity**, which states that the variance of the residuals should remain constant across all levels of the independent variables. We assessed homoscedasticity by examining scatterplot of standardized residuals against predicted values.  Ideally, these residuals should be evenly distributed around zero; any patterns may indicate the presence of heteroscedasticity.

The assumption of **no multicollinearity** asserts that the predictor variables should not be highly correlated with each other, as this can inflate the variance of the coefficient estimates and make them unstable. To check for multicollinearity, we created a correlation matrixfor the predictors to look for any correlations greater than 0.8 (or less than -0.8). 

Finally, it is important to consider the **ratio of observations to predictors** in the model. A general guideline suggests having at least 10 to 15 observations for each predictor to ensure robust estimates. Since we have over 1700 observations in our dataset, this assumption is met. 

After verifying these assumptions, we proceed with the regression analysis. There are several parameters that we need to estimate here: \( \beta_0 \), which is the intercept,  \( \beta_1, \dots, \beta_k \), which are coefficients of the independent variables, as well as  \( \sigma^2 \), the variance of the error terms, which represents the variability in the dependent variable \(Y\) that is not explained by the predictors \(X_1, \dots, X_k\).

The least squares method estimates the coefficients by *minimizing the sum of squared residuals*. Given \(n\) observations on \(y\), and \(k\) predictors  \(X_1, \dots, X_k\), the estimates \( \beta_1, \dots, \beta_k \) are chosen simultaneously to minimize the expression for the Error Sum of Squares (SSE), given by:

$$
\text{SSE} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \sum_{i=1}^{n} (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_{i1} - \dots - \hat{\beta}_k x_{ik})^2
$$

where \( y_i \) represents the actual values, \( \hat{\beta}_0 \) is the estimated intercept, \( \beta_1 \) is the coefficient for the predictor \(x_{1i}\), and \( x_{1i} \) is the predictor value for the \(i\)-th observation.

With the Error Sum of Squares (SSE), the equation of the estimated variance of the error term \( \sigma^2 \) is given by:

$$
\sigma^2 = \frac{\text{SSE}}{n - (k+1)}
$$

where \( n \) is the number of observations and \( k \) is the number of predictors in the model.

We evaluated the model's goodness of fit using the coefficient of determination \(R^2\), which quantifies the proportion of variance in the dependent variable explained by the independent variables. The adjusted \(R^2\), which adjusts the \(R^2\) value based on the number of predictors in the model to provide a more accurate measure of model fit for multiple regression. In simple regression: 

$$
SST = \sum_{i=1}^{n} (y_i - \bar{y})^2
$$
where \( y_i \) is the observed value, \( \bar{y} \) is the mean of the observed values, and \( n \) is the number of observations. Then,   \(R^2\) can be simply obtained by: 

$$
R^2 = 1 - \frac{SSE}{SST}
$$
\(R^2\) is then typically adjusted as follows based on \( n \), the number of observations and \( k \), the number of predictors in the model. 

$$
R^2_{\text{adj}} = 1 - \frac{(1 - R^2)(n - 1)}{n - k - 1}
$$

In this analysis, we test several hypotheses to evaluate the significance of the model and its predictors. The overall significance of the regression model is assessed using the F-ratio. It compares the variance explained by the regression model to the variance that is not explained by the model, assessing whether the independent variables as a whole have a statistically significant effect on the dependent variable. A higher F-ratio indicates that the regression model explains a significant amount of variance in the dependent variable compared to the residual variance.

The null hypothesis (\( H_0 \)) and alternative hypothesis (\( H_a \)) for the F-ratio are stated as follows:

- **Null Hypothesis (\( H_0 \))**: The regression model does not explain a significant portion of the variance in the dependent variable (median house value). In our case, this means that the coefficients for all the predictors are equal to zero and none of them explain any variance in housing price. This can be formally stated as:
  $$
  H_0: \beta_1 = \beta_2 = \beta_3 = \beta_4 = 0
  $$

- **Alternative Hypothesis (\( H_a \))**: At least one of the predictor variables has a significant effect on the dependent variable. In our case, this means that at least one of the coefficients for the predictors is not equal to zero, indicating that the predictor has a significant impact on housing prices. This can be stated as:
  $$
  H_a: \text{At least one } \beta_i \neq 0
  $$

For each individual predictor \( \beta_i \), we also test the following hypotheses:

- **Null Hypothesis (\( H_{0i} \))**: The coefficient for the predictor \( i \) is equal to zero, indicating that it does not have a significant effect on the dependent variable (median house value). This can be formally stated as:
  $$
  H_{0i}: \beta_i = 0
  $$

- **Alternative Hypothesis (\( H_{ai} \))**: The coefficient for the predictor \( i \) is not equal to zero, indicating that it has a significant effect on the dependent variable. This can be stated as:
  $$
  H_{ai}: \beta_i \neq 0
  $$

Each predictor's significance is typically assessed using a t-test, which tests whether the estimated coefficient differs significantly from zero.

## Additional Analyses - Emily

Talk about stepwise regression â€“ discuss what it does and its limitations

Talk about k-fold cross-validation (mentioning that k = 5) â€“ discuss what it is used for, describe how it is operationalized and mention that the RMSE is used to compare models (explain what the RMSE is and how it is calculated, presenting and describing any relevant formulas).

## Software and Pacakges 

For our data analysis, we are utilizing the R programming language. The following libraries have been employed to facilitate our analysis: 

- `tidyverse`: A collection of R packages designed for data science that includes tools for data manipulation, visualization, and analysis.
- `here`: A package that simplifies file path management, making it easier to work with project directories.
- `kableExtra`: A package for creating and customizing tables in R Markdown documents.
- `ggplot2`: A widely-used package for creating static graphics and visualizations based on the grammar of graphics.
- `ggcorrplot`: A package specifically designed for visualizing correlation matrices using ggplot2.
- `sf`: A package that provides support for spatial data in R, enabling the handling and analysis of geometric objects.
- `patchwork`: A package that allows for the combination of multiple ggplot2 plots into a single cohesive layout.
- `MASS`: A package that provides functions and datasets for various statistical methods, including linear and generalized linear models.
- `caret`: A package that streamlines the process of creating predictive models, including functions for data splitting, pre-processing, feature selection, and model tuning.


# Results

## Exploratory Results - Ziyi

```{r mean and sd, message=FALSE, warning=FALSE}

# examine the mean and standard deviation of dependent and independent variables. 
dist_results <- data.frame(Variable = character(), Mean = numeric(), SD = numeric(), stringsAsFactors = FALSE)
variables <- c("MEDHVAL", "PCTBACHMOR", "NBELPOV100", "PCTVACANT", "PCTSINGLES")
relabelled_variables <- c(
  "Median House Value",                                    # MEDHVAL
  "% of Individuals with Bachelorâ€™s Degrees or Higher",    # PCTBACHMOR
  "# Households Living in Poverty",                        # NBELPOV100
  "% of Vacant Houses",                                    # PCTVACANT
  "% of Single House Units"                                # PCTSINGLES
)

for (i in seq_along(variables)) {
  
  mean_val <- round(mean(data[[variables[i]]], na.rm = TRUE), 3) 
  sd_val <- round(sd(data[[variables[i]]], na.rm = TRUE), 3) 
  
  # Store the relabeled variable names
  dist_results <- rbind(dist_results, data.frame(Variable = relabelled_variables[i], Mean = mean_val, SD = sd_val))
}


dist_results <- rbind(
  data.frame(Variable = "Dependent Variable", Mean = "", SD = ""),
  data.frame(Variable = "Median House Value", Mean = dist_results$Mean[1], SD = dist_results$SD[1]),
  data.frame(Variable = "Predictors", Mean = "", SD = ""),
  dist_results[-1, ] # Remove the first row because it has already been used above
)

dist_results %>%  
  kable(row.names = FALSE) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>% 
  row_spec(0, bold = TRUE) %>%  # Bold the header row
  row_spec(1, bold = TRUE) %>%  # Bold the 'Dependent Variable' row
  row_spec(3, bold = TRUE)


```

We began by summarizing the statistical data for median house values and various socio-economic predictors. The median house value has an average of \$66,287.73, with a standard deviation of \$60,006.08. This is significantly lower than both the county and state averages and shows considerable variation in house values. For educational attainment, 16.08% of individuals hold a bachelorâ€™s degree or higher, much lower than the state average of 35.3%. The standard deviation of 17.77% suggests notable variability in this measure. The average number of households living in poverty is 189.77, with a wide range reflected by a standard deviation of 164.32. Vacant houses make up an average of 11.29%, slightly higher than the state average of 9.4%, with a standard deviation of 9.63%. Finally, single house units account for an average of 9.23%, with a standard deviation of 13.25%. 


```{r histograms, fig.height=7, fig.width=9, message=FALSE, warning=FALSE}

# histogram

data %>%
  pivot_longer(cols = c("MEDHVAL", "PCTBACHMOR", "NBELPOV100", "PCTVACANT", "PCTSINGLES"),
               names_to = "Variable",
               values_to = "Value") %>% 
  ggplot(aes(x = Value)) +
  geom_histogram(aes(y = ..count..), fill = "#283d3b", alpha = 0.7) +  
  facet_wrap(~Variable, scales = "free", ncol = 3, labeller = as_labeller(c(
    "MEDHVAL" = "Median House Value",
    "PCTBACHMOR" = "% with Bachelorâ€™s Degrees or Higher",
    "NBELPOV100" = "# Households Living in Poverty",
    "PCTVACANT" = "% of Vacant Houses",
    "PCTSINGLES" = "% of Single House Units"
  ))) +  
  labs(x = "Value", y = "Count", title = "Histograms of Dependent and Predictor Variables") +
  theme_light() +   
  theme(plot.subtitle = element_text(size = 9,face = "italic"),
        plot.title = element_text(size = 12, face = "bold"), 
        axis.text.x=element_text(size=6),
        axis.text.y=element_text(size=6), 
        axis.title=element_text(size=8))




```

According to the histograms above, the original distributions were not normal. To address this, we applied a logarithmic transformation to the variables. 

```{r log variables, message=FALSE, warning=FALSE}

# log the variables

reg_data <- data %>%
  mutate(
    LNMEDHVAL = if (min(MEDHVAL) == 0) log(1 + MEDHVAL) else log(MEDHVAL),
    LNPCTBACHMOR = if (min(PCTBACHMOR) == 0) log(1 + PCTBACHMOR) else log(PCTBACHMOR),
    LNNBELPOV100 = if (min(NBELPOV100) == 0) log(1 + NBELPOV100) else log(NBELPOV100),
    LNPCTVACANT = if (min(PCTVACANT) == 0) log(1 + PCTVACANT) else log(PCTVACANT),
    LNPCTSINGLES = if (min(PCTSINGLES) == 0) log(1 + PCTSINGLES) else log(PCTSINGLES)
  )

```


```{r logged histograms, fig.height=7, fig.width=9, message=FALSE, warning=FALSE}

# histograms of the logged variables
reg_data %>%
  pivot_longer(cols = c("LNMEDHVAL", "LNPCTBACHMOR", "LNNBELPOV100", "LNPCTVACANT", "LNPCTSINGLES"),
               names_to = "Variable",
               values_to = "Value") %>% 
  ggplot(aes(x = Value)) +
  geom_histogram(aes(y = ..count..), fill = "#283d3b", alpha = 0.7) +  
  facet_wrap(~Variable, scales = "free", ncol = 3, labeller = as_labeller(c(
    "LNMEDHVAL" = "Median House Value",
    "LNPCTBACHMOR" = "% with Bachelorâ€™s Degrees or Higher",
    "LNNBELPOV100" = "# Households Living in Poverty",
    "LNPCTVACANT" = "% of Vacant Houses",
    "LNPCTSINGLES" = "% of Single House Units"
  ))) +  
  labs(x = "Value", y = "Count", title = "Histograms with Logged Transform Variables") +
  theme_light() +   
  theme(plot.subtitle = element_text(size = 9,face = "italic"),
        plot.title = element_text(size = 12, face = "bold"), 
        axis.text.x=element_text(size=6),
        axis.text.y=element_text(size=6), 
        axis.title=element_text(size=8))

```

This transformation has helped normalize some of the distributions, particularly for variables like median house value, number of households in poverty, and percentage of vacant houses. These variables now display more symmetric, bell-shaped distributions. However, for variables like the percentage of individuals with a bachelorâ€™s degree and percentage of single house units, the transformation was less effective, and they still deviate from a normal distribution. 

Moving forward, we will use the log-transformed results for our analysis. The remaining regression assumptions will be examined in detail in a separate section titled Regression Assumption Checks.


```{r corr plot, message=FALSE, warning=FALSE}

custom_labels <- c(
  "% of Individuals with Bachelorâ€™s Degrees or Higher" = "PCTBACHMOR",
  "% of Vacant Houses" = "PCTVACANT",
  "% of Single House Units" = "PCTSINGLES",
  "# Households Living in Poverty" = "LNNBELPOV100"
)

cor_matrix <- cor(reg_data %>% dplyr::select(PCTBACHMOR, PCTVACANT, PCTSINGLES, LNNBELPOV100))
rownames(cor_matrix) <- names(custom_labels)
colnames(cor_matrix) <- names(custom_labels)

ggcorrplot(cor_matrix, 
           type = "lower", 
           lab = TRUE, 
           lab_size = 3, 
           colors = c("#283d3b", "white", "#c44536")) +
  labs(title = "Correlation Matrix for all Predictor Variables") +
  theme(plot.subtitle = element_text(size = 9, face = "italic"),
        plot.title = element_text(size = 12, face = "bold"), 
        axis.text.x = element_text(size = 7),
        axis.text.y = element_text(size = 7), 
        axis.title = element_text(size = 8))
```

The choropleth maps of the dependent variable and the predictor variables reveal several notable patterns. The map displaying the percentage of individuals with a bachelor's degree or higher shows similarities to the map depicting the percentage of single-family housing units, both highlighting higher concentrations in the northwest, with additional clusters in Center City and the far northeast. In contrast, the maps illustrating households living in poverty and vacant housing share similar patterns, showing higher concentrations in the north, west, and parts of the south of the city. This suggests an inverse relationship between poverty and vacant housing with educational attainment and single-family homeownership, which is consistent with the correlation matrix.

Although some pairs of independent variables share general spatial patterns, they do not appear to be strongly correlated, and no severe multicollinearity is assumed. The correlation matrix validates this assumption, as no predictor pairs show very high correlations. 

Moreover, the dependent variable, median housing value, appears to follow a pattern similar to educational attainment and single-family homeownership, while demonstrating an opposite trend to poverty and vacant housing. Based on these visualizations, we can expect educational attainment, specifically the percentage of individuals with a bachelor's degree or higher, to have the strongest association with the dependent variable, as these maps exhibit the most similar patterns.

Present the choropleth maps of the dependent variable and the predictors. Refer to the maps in the text, and talk about the following: Which maps look similar? Which maps look different? That is, which predictors do you expect to be strongly associated with the dependent variable based on the visualization? Also, given your examination of the maps, are there any predictors that you think will be strongly inter-correlated? That is, do you expect severe multicollinearity to be an issue here? Discuss this in a paragraph.

Does the correlation matrix support your conclusions based on your visual comparison of predictor maps? 

```{r choropleth, message=FALSE, warning=FALSE}

# choropleth maps of predictor and dependent variables
philly <- st_read(here("data","shapefile", "RegressionData.shp"))
ggplot(philly) +
  geom_sf(aes(fill = LNMEDHVAL), color = "transparent") +
  scale_fill_gradientn(colors = c("#FAF9F6", "#c44536"), 
                       name = "LNMEDHVAL", 
                       na.value = "transparent") +  # Handle NAs
  theme(legend.text = element_text(size = 9),
        legend.title = element_text(size = 10),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        plot.subtitle = element_text(size = 9, face = "italic"),
        plot.title = element_text(size = 12, face = "bold"),
        panel.background = element_blank(),
        panel.border = element_rect(colour = "grey", fill = NA, size = 0.8)) +
  labs(title = "Log Transformed Median House Value")

```

```{r more choropleth, fig.height=11, fig.width=14, message=FALSE, warning=FALSE}

map_pctvacant <- ggplot(philly) +
  geom_sf(aes(fill = PCTVACANT), color = "transparent") +
  scale_fill_gradientn(colors = c("#FAF9F6", "#c44536"), 
                       name = "PCTVACANT", 
                       na.value = "transparent") +  # Handle NAs
  theme(legend.text = element_text(size = 9),
        legend.title = element_text(size = 10),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        plot.subtitle = element_text(size = 9, face = "italic"),
        plot.title = element_text(size = 12, face = "bold"),
        panel.background = element_blank(),
        panel.border = element_rect(colour = "grey", fill = NA, size = 0.8)) +
  labs(title = "% of Vacant Houses")

map_pctsingles <- ggplot(philly) +
  geom_sf(aes(fill = PCTSINGLES), color = "transparent") +
  scale_fill_gradientn(colors = c("#FAF9F6", "#c44536"), 
                       name = "PCTSINGLES", 
                       na.value = "transparent") +  # Handle NAs
  theme(legend.text = element_text(size = 9),
        legend.title = element_text(size = 10),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        plot.subtitle = element_text(size = 9, face = "italic"),
        plot.title = element_text(size = 12, face = "bold"),
        panel.background = element_blank(),
        panel.border = element_rect(colour = "grey", fill = NA, size = 0.8)) +
  labs(title = "% of Single House Units")

map_pcbachmore <- ggplot(philly) +
  geom_sf(aes(fill = PCTBACHMOR), color = "transparent") +
  scale_fill_gradientn(colors = c("#FAF9F6", "#c44536"), 
                       name = "PCTBACHMOR", 
                       na.value = "transparent") +  # Handle NAs
  theme(legend.text = element_text(size = 9),
        legend.title = element_text(size = 10),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        plot.subtitle = element_text(size = 9, face = "italic"),
        plot.title = element_text(size = 12, face = "bold"),
        panel.background = element_blank(),
        panel.border = element_rect(colour = "grey", fill = NA, size = 0.8)) +
  labs(title = "% Bachelor's Degree or Higher")


map_lnbelpov100 <- ggplot(philly) +
  geom_sf(aes(fill = LNNBELPOV), color = "transparent") +
  scale_fill_gradientn(colors = c("#FAF9F6", "#c44536"), 
                       name = "LNNBELPOV", 
                       na.value = "transparent") +  # Handle NAs
  theme(legend.text = element_text(size = 9),
        legend.title = element_text(size = 10),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        plot.subtitle = element_text(size = 9, face = "italic"),
        plot.title = element_text(size = 12, face = "bold"),
        panel.background = element_blank(),
        panel.border = element_rect(colour = "grey", fill = NA, size = 0.8)) +
  labs(title = "Logged Transformed Poverty")

map_pctvacant + map_pctsingles + map_pcbachmore + map_lnbelpov100 + 
  plot_layout(ncol = 2)
```

## Regression Results - Emma
*Present the regression output from R. Be sure that your output presents the parameter estimates (and associated standard errors, t-statistics and p-values), as well as the R2, the adjusted R2, and the relevant F-ratio and associated p-value.

*Referencing the regression output in (i) above, interpret the results as in the example included above this report outline.

Four independent variables, the proportion of vacant housing units (PCTVACANT), the proportion of single-family detached homes (PCTSINGLES), the proportion of residents with at least a Bachelor's degree (PCTBACHMOR), and the number of log-transformed households living below the poverty line (LNNBELPOV100), were used to construct ordinary least squares (OLS) regression models to predict the median log-transformed home values (LNMEDHVAL) for the Philadelphia census block group. The results of the regression analysis reveal a link between these block characteristics and house prices.

The residuals from the model show a fairly reasonable distribution of errors, with a minimum residual value of -2.26, a first quartile of -0.20, a median of 0.04, a third quartile of 0.22, and a maximum value of 2.24. This distribution suggests that the model captures changes in housing values reasonably well, although some of the extreme residuals suggest that outliers may exist.

The regression coefficients provide further evidence of the effect of each predictor on LNMEDHVAL. The intercept of the model is 11.11 (p < 0.001), indicating the expected log-transformed median home value when all predictors are held constant at zero. The proportion of vacant housing units (PCTVACANT) had a highly significant negative effect on LNMEDHVAL (Î² = -0.019, p < 0.001), suggesting that the higher the vacancy rate, the lower the median housing value. Specifically, for every 1% increase in vacant housing units, the log-transformed median home value decreases by 0.019 units, highlighting the negative impact of vacancy on property values.

The proportion of single-family detached homes (PCTSINGLES) is significantly and positively correlated with LNMEDHVAL (Î² = 0.003, p < 0.001). This suggests that neighbourhoods with more single-family homes tend to have higher median housing values. Although the effect size is small, it is still statistically significant, suggesting that the presence of single-family homes contributes positively to community home values.

The proportion of residents with at least a bachelor's degree (PCTBACHMOR) has the largest positive effect on LNMEDHVAL (Î² = 0.021, p < 0.001), suggesting that populations with higher levels of educational attainment tend to reside in areas with higher housing values. Each 1 percent increase in the proportion of residents with a bachelor's degree is associated with a 0.021 unit increase in the log-transformed median home value, underscoring the importance of educational attainment as a key factor in housing market dynamics.

In contrast, the number of log-transformed households living below the poverty line (LNNBELPOV100) is negatively correlated with LNMEDHVAL (Î² = -0.079, p < 0.001). This result suggests that higher levels of poverty are associated with lower home values in a neighbourhood group, with each 1 per cent increase in the number of households below the poverty line being associated with a 0.079 unit decrease in the median log-transformed home value.

The model had a multiple R-squared value of 0.6623 and an adjusted R-squared value of 0.6615, indicating that approximately 66% of the variance in LNMEDHVAL is explained by the four predictors. The F-statistic for this model was 840.9 (p < 0.001), further confirming the statistical significance of the model as a whole, and the fact that the predictors collectively explain a significant portion of the variance in housing values for the Philadelphia block group.

The analysis of variance (ANOVA) table provides additional insight into the significance of each predictor in explaining the variation in LNMEDHVAL. The ANOVA decomposes the total variation in LNMEDHVAL into components attributable to each predictor and residual error. For PCTVACANT, the sum of squares is 180.39, with a mean square value of 180.39, and an F-statistic of 1343.09 (p < 0.001). This result confirms that housing vacancy is a significant factor contributing to the variation in housing values. Similarly, PCTSINGLES has a sum of squares of 24.54 and an F-statistic of 182.73 (p < 0.001), confirming the significant, though smaller, positive effect of single-family homes on housing values.

The PCTBACHMOR variable is the strongest predictor, with a sum of squares of 235.12 and an F-statistic of 1750.55 (p < 0.001). This reinforces the importance of educational attainment in driving housing values. Lastly, LNNBELPOV100 has a sum of squares of 11.69 and an F-statistic of 87.05 (p < 0.001), showing that poverty levels, while statistically significant, have a more modest effect compared to other predictors.

The residual sum of squares, which represents the unexplained variation in the model, was 230.34, with a mean square error of 0.134.Overall, the ANOVA results, combined with the OLS regression results, confirmed that all four predictors-PCTVACANT, PCTSINGLES, PCTBACHMOR, and LNNBELPOV100-contribute significantly to explaining the changes in LNMEDHVAL were all significant contributors. These findings provide strong evidence of the robustness of the model and the relevance of neighbourhood characteristics in predicting housing values.

```{r linear regression, message=FALSE, warning=FALSE}

fit <- lm(LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV100, data=reg_data)
summary(fit)
```


```{r anova table, message=FALSE, warning=FALSE}

anova_table <- anova(fit)
anova_table

```


## Regression Assumption Check - Emma

*First state that in this section, you will be talking about testing model assumptions. State that you have already looked at the variable distributions earlier.

*Present scatter plots of the dependent variable and each of the predictors.  State whether each of the relationships seems to be linear, as assumed by the regression model. [Hint: they will not look linear.]

The scatter plot for PCTVACANT shows a general negative relationship between the proportion of vacant housing units and LNMEDHVAL. In the graph, there are clusters of block groups where high vacancy rates correspond to lower housing values, particularly on the left side of the plot where the values of LNMEDHVAL are higher. However, the plot is not perfectly linear. Instead, it reveals a more complex relationship with significant variation in housing values across different vacancy rates. In the middle and lower ranges of vacancy rates, the decline in housing values becomes less pronounced. This suggests that while vacant housing is associated with lower property values, the impact diminishes or becomes less predictable as the vacancy rate changes.

In the scatter plot for PCTSINGLES, a weakly positive trend is observed, though the points are scattered widely around the line, indicating a weak linear relationship. As the percentage of single-family homes increases, the values of LNMEDHVAL tend to increase, but this pattern is not consistent across all block groups. The scatter of points shows significant variability in housing values at various levels of single-family housing, with some block groups exhibiting high values even at lower percentages of single-family homes. This variability suggests that while there is a positive association between single-family housing and higher values, the relationship is more nuanced, and other factors may be influencing house values at the same time.

The scatter plot for PCTBACHMOR, however, shows a much clearer linear relationship. As the proportion of residents with at least a bachelorâ€™s degree increases, the LNMEDHVAL values increase in a more consistent and linear fashion. The points on the graph form a relatively tight band along a positive trend line, indicating that higher education levels within a neighborhood are strongly associated with higher housing values. The consistent upward trend in this graph suggests that educational attainment is a key driver of housing prices, and the linear relationship implies that this variable is well-suited for inclusion in the linear regression model.

The scatter plot for LNNBELPOV100 reveals a predominantly negative relationship, but like PCTVACANT, the association is not strictly linear. In the graph, as the number of households living below the poverty line increases, the LNMEDHVAL values generally decrease. However, the points are scattered widely, particularly at lower levels of poverty, where housing values vary significantly. This indicates that while poverty levels have a negative impact on housing values, the effect is not uniform across all block groups. There is substantial variation in the housing values even in neighborhoods with similar poverty rates, suggesting that other factors may be at play. The non-linear pattern observed in this graph highlights the complexity of povertyâ€™s influence on the housing market.

In summary, the scatterplot shows that while PCTBACHMOR and LNMEDHVAL exhibit a strong and clear linear relationship, the other predictors - in particular PCTVACANT and LNNBELPOV100 - exhibit more complex non-linear patterns. The scatterplots provide important insights into the relationship between the independent variables and housing values, suggesting that while the model is assumed to be linear, it may not fully capture the underlying dynamics. For variables such as PCTVACANT and LNNBELPOV100, the nonlinearity observed in the plots suggests that more complex nonlinear models may provide a better fit and more accurate predictions.

```{r scatterplot, fig.height=7, fig.width=10, message=FALSE, warning=FALSE}

# scatterplot

reg_data %>%
  pivot_longer(cols = c("PCTBACHMOR", "LNNBELPOV100", "PCTVACANT", "PCTSINGLES"),
               names_to = "Variable",
               values_to = "Value") %>% 
ggplot(aes(x = Value, y = LNMEDHVAL)) +
  geom_point(color = "#283d3b", alpha = 0.7, size = 0.4) +
  geom_smooth(method = "lm", color = "#c44536", se = FALSE) +  # Add a linear regression line
  facet_wrap(~ Variable, scales = "free", labeller = as_labeller(c(
    "PCTBACHMOR" = "% with Bachelorâ€™s Degrees or Higher",
    "LNNBELPOV100" = "Households Living in Poverty",
    "PCTVACANT" = "% of Vacant Houses",
    "PCTSINGLES" = "% of Single House Units"
  )))  +
  theme_light() +   
  theme(plot.subtitle = element_text(size = 9,face = "italic"),
        plot.title = element_text(size = 12, face = "bold"), 
        axis.text.x=element_text(size=6),
        axis.text.y=element_text(size=6), 
        axis.title=element_text(size=8)) +
  labs(title = "Scatter Plots of Dependent Variable vs. Predictors", 
       x = "Predictor Value", 
       y = "Log of Median House Value")

```

*Present the histogram of the standardized residuals. State whether the residuals look normal.

The histogram of standardized residuals provides a detailed view of the distribution of the residuals from the OLS regression model. This graph presents the frequency of residual values along the x-axis, with the residuals centered around zero. The shape of the histogram approximates a bell curve, which is characteristic of a normal distributionâ€”a key assumption for OLS regression.

The majority of the residuals are concentrated between -2 and 2 on the x-axis, with the highest frequency occurring near zero. This concentration suggests that most of the predicted values are close to the observed values, leading to residuals that are small and distributed symmetrically around zero. This is a positive sign, as it indicates that the model does not systematically overpredict or underpredict the dependent variable (LNMEDHVAL).

At the center of the distribution, the residuals form a tall, narrow peak, representing a large number of residuals close to zero. This suggests that the model fits most observations well. As we move further away from the center, the frequency of residuals decreases symmetrically, with fewer residuals having large positive or negative values. This tapering off on both sides of the histogram suggests that extreme residuals are relatively rare, which is typical of a normal distribution.

However, there are some noticeable deviations from perfect normality at the tails of the distribution. On the left side of the graph, there are a few residuals with values smaller than -3, and on the right side, there are a few residuals larger than 3. These residuals represent outliersâ€”cases where the modelâ€™s predictions deviate significantly from the observed values. While these extreme values do not appear to dominate the distribution, they are worth further examination, as they could indicate areas where the modelâ€™s assumptions are being violated or where influential points might be affecting the overall model fit.

In summary, the histogram of standardized residuals indicates that the residuals are approximately normally distributed, which supports the assumption of normality for the regression model. Although there are some outliers present, their overall impact on the normality of the residuals seems minimal. The general shape of the histogram suggests that the OLS regression model fits the data reasonably well, with residuals mostly falling within an acceptable range around zero. 

```{r compute residuals, message=FALSE, warning=FALSE}

fitted_values <- fitted(fit)
residuals_values <- residuals(fit)
standardized_residuals <- rstandard(fit)

reg_data <- reg_data %>%
  mutate(
    Fitted = fitted_values,
    Residuals = residuals_values,
    Standardized_Residuals = standardized_residuals)

```


```{r residual histograms, message=FALSE, warning=FALSE}

ggplot(reg_data, aes(x = Standardized_Residuals)) +
  geom_histogram(bins = 30, fill = "#283d3b", alpha = 0.7) +
  labs(title = "Histogram of Standardized Residuals", 
       x = "Standardized Residuals", 
       y = "Frequency") +
  theme_light() +   
  theme(plot.subtitle = element_text(size = 9,face = "italic"),
        plot.title = element_text(size = 12, face = "bold"), 
        axis.text.x=element_text(size=6),
        axis.text.y=element_text(size=6), 
        axis.title=element_text(size=8))

```


*Present the â€˜Standardized Residual by Predicted Valueâ€™ scatter plot. What conclusions can you draw from that? Does there seem to be heteroscedasticity? Do there seem to be outliers? Anything else? Discuss. 
Mention what standardized residuals are.

The scatter plot of standardized residuals vs fitted values is a valuable diagnostic for testing essential regression assumptions, including homoscedasticity and the presence of outliers. The fitted values that the model predicts are plotted on the x-axis, and the standardized residualsâ€”that is, residuals adjusted by their standard deviationâ€”are shown on the y-axis. Because standardized residuals give an indication of the standard deviation of each residual's divergence from the expected value, they make it simple to identify outliers and significant data points. Relatives are generally regarded as normal when they fall between -2 and +2, and anything outside of these bounds can lead to anomalous points that could have an impact on the model's performance.

Homoscedasticity, or the need that residuals show constant variance across all levels of fitted values, is one of the fundamental presumptions of regression. The residuals in this scatter plot show no discernible trend of rising or falling variance with increasing fitted values; instead, they seem to be pretty uniformly distributed around the horizontal line at zero. The fact that there is no obvious funnel shape or systematic variation trend to suggest otherwise suggests that the assumption of homoscedasticity is reasonably satisfied. The notion of constant variance is supported by the residuals' generally constant spread across the fitted value range, which indicates that the variance of errors is still stable.

However, the plot does show some potential outliers, particularly those points that are outside the range of the -3 and +3 standardised residuals. These extreme residuals are located above and below the main residual clusters and may indicate that the model's predicted values deviate significantly from the actual values. Although the number of outliers is relatively small, their presence indicates that the model may not fit all data points well. Outliers such as these may have a disproportionate impact on the model, potentially distorting parameter estimates and exaggerating error terms. Further investigation is needed to determine whether these outliers are due to data entry errors, represent unique situations, or highlight deficiencies in the model specification.

The symmetry of the residuals around the zero line is another positive aspect of the plot. This symmetry means that the model does not systematically over- or under-predict the dependent variable over the range of fitted values (LNMEDHVAL). The symmetrical distribution of the residuals is consistent with a key assumption of ordinary least squares regression, which is that the residuals should have a mean of zero. The lack of significant bias in either direction suggests that the model is generally appropriate.

In addition, although the residuals are generally well clustered around zero, some slight dispersion occurs as the fitted values increase, suggesting that the accuracy of the model predictions may decrease slightly at higher values of LNMEDHVAL. The higher the fitted value, the greater the residual variance, which may indicate the presence of slight heteroskedasticity, but not of sufficient severity to raise significant concerns about the validity of the model. However, this pattern could be explored further to ensure that the model performs well over the entire range of predicted values.

Thus, scatter plots of standardised residuals versus fitted values indicate that the model generally satisfies the assumption of homoskedasticity and that the residuals show a consistent distribution among the fitted values. However, the presence of a few outliers and slight dispersion of higher values warrants further investigation. These diagnostic results suggest that the model fits the data reasonably well, but dealing with outliers and checking for leverage points could improve the robustness and predictive accuracy of the model.


```{r residual plot,  message=FALSE, warning=FALSE}

ggplot(reg_data, aes(x = Fitted, y = Standardized_Residuals)) +
  geom_point(color = "#283d3b", alpha = 0.6) +    # Scatter plot points
  geom_hline(yintercept = 0, linetype = "dashed", color = "#c44536") +   # Add a horizontal line at y = 0
  labs(
    title = "Scatter Plot of Standardized Residuals vs Fitted Values",
    x = "Fitted Values",
    y = "Standardized Residuals"
  ) +
  theme_light() +   
  theme(plot.subtitle = element_text(size = 9,face = "italic"),
        plot.title = element_text(size = 12, face = "bold"), 
        axis.text.x=element_text(size=6),
        axis.text.y=element_text(size=6), 
        axis.title=element_text(size=8))


```

*Referencing the maps of the dependent variable and the predictors that you presented earlier, state whether there seems to be spatial autocorrelation in your variables. That is, does it seem that the observations (i.e., block groups) are independent of each other? Briefly discuss.

In the map of LNMEDHVAL (log-transformed median house values), there appears to be a spatial pattern, with higher values concentrated in certain areas, such as Center City and other affluent neighborhoods, and lower values clustered in more economically distressed areas. This suggests that the housing market is not spatially random and that nearby block groups tend to have similar house values, indicative of positive spatial autocorrelation.

Similarly, the map of PCTVACANT (proportion of vacant housing units) shows clear geographic clusters, with higher vacancy rates concentrated in certain parts of the city, particularly in areas experiencing economic decline. This clustering pattern suggests that vacancy rates are spatially correlated, meaning that the vacancy rate in one block group is likely influenced by the rates in adjacent block groups.

For PCTSINGLES (Proportion of single-family homes), the maps also reveal spatial patterns, with block groups characterised by a high proportion of single-family homes, often located in residential areas on the outskirts of a suburb or city. This spatial clustering implies that block groups with similar housing characteristics tend to be in close proximity to each other, suggesting potential spatial autocorrelation.

The PCTBACHMOR (Proportion of Residents with at least a Bachelor's Degree) maps also show a strong spatial pattern, with higher levels of educational attainment concentrated in the more affluent central areas of the city, while lower levels of educational attainment are concentrated in the peripheral areas, which have fewer economic resources. This suggests that educational attainment is spatially correlated, with similar levels of education concentrated in geographically similar neighbourhood groups.

The LNNBELPOV100 (log-transformed number of households living below the poverty line) map shows that in some parts of the city, particularly in economically distressed neighbourhoods, there is a clear concentration of households with higher poverty rates. This spatial concentration of poverty is indicative of positive spatial autocorrelation, whereby groups of neighbourhoods with higher poverty rates tend to be in close proximity to each other.

In summary, the maps of both the dependent variable and the predictors indicate the presence of spatial autocorrelation, with distinct geographic patterns and clusters visible throughout the city. This suggests that observations (block groups) are not independent of each other, as values from nearby block groups are likely to influence each other. The presence of spatial autocorrelation highlights the importance of considering spatial relationships in the analysis, as failure to account for such dependencies may violate regression assumptions and lead to biased or inefficient estimates. The use of spatial regression techniques to better account for these spatial dependencies could be of great benefit to future analyses.

*Now, present the choropleth map of the standardized regression residuals. Do there seem to be any noticeable spatial patterns in them? That is, do they seem to be spatially autocorrelated? You will examine the spatial autocorrelation of the variables and residuals and run spatial regressions in the next assignment. 

The choropleth map of standardized regression residuals provides a detailed geographic breakdown of how well the OLS regression model fits the data across different areas of the city. The map uses a color gradient to represent the magnitude of the residuals, with darker red shades indicating large positive residuals (where the model underestimates the actual values), and lighter shades (toward white) indicating large negative residuals (where the model overestimates the actual values). The standardized residuals range from -6 to +6, offering insight into areas where the model's predictions diverge most from the observed values.

From a spatial perspective, there are several noteworthy patterns in the map. In the northwestern part of the city, particularly along some of the more suburban and affluent areas, dark red shading indicates large positive residuals. This suggests that the model underestimates housing values in these neighborhoods, as the predicted values are consistently lower than the actual observed values. These areas are often characterized by higher-income households and larger single-family homes, suggesting that the model may not fully capture the higher market value trends in these regions, possibly due to unaccounted factors like proximity to high-demand amenities or specific neighborhood features that influence property values.

In contrast, in sections of South and Southwest Philadelphia, as well as parts of North Philadelphia, lighter shaded areas indicate large negative residuals. Here, the model tends to overestimate housing values, meaning that the predicted values are higher than the actual observed values. These neighborhoods may be facing economic challenges, such as higher vacancy rates, lower household incomes, or poverty, which the model has difficulty fully capturing through the selected predictors. The residuals in these areas suggest that housing markets in these neighborhoods are more complex and influenced by localized factors not fully represented in the regression model, such as higher levels of economic distress or neighborhood disinvestment.

Additionally, some central neighborhoods near Center City display more neutral residuals, where the model's predictions are relatively accurate, with values closer to zero. These areas appear in lighter red or pink shades, indicating that the model is performing reasonably well in these parts of the city. This may be because the central areas are more homogeneous in terms of housing values and more likely to align with the trends captured by the model's predictors, such as education levels and vacancy rates.

The overall pattern observed in the map suggests spatial clustering, as block groups with similar residual values tend to be geographically close to each other. For instance, in the northwest, large positive residuals form a contiguous cluster, while large negative residuals are clustered in certain distressed areas of the city. This clustering effect points to the presence of spatial autocorrelation in the residuals, meaning that the model's errors are not randomly distributed, but instead show clear geographic trends.

In other words, therefore, the longitudinal plots of standardised residuals reveal significant spatial patterns, suggesting that the regression model performs differently across communities. The model tends to underestimate housing values in more affluent suburbs and overestimate housing values in economically distressed communities. The spatial pattern of these residuals suggests that there are other geographic or spatial factors that influence housing values that are not fully accounted for in the model. This highlights the need for further spatial analyses, such as incorporating spatial regression models, to account for these dependencies and improve the accuracy of the models. By addressing spatial autocorrelation, future models could provide more accurate forecasts and better insight into the spatial dynamics of housing markets across the city.

```{r residual maps, message=FALSE, warning=FALSE}

philly %>%
  left_join(reg_data %>% dplyr::select(c(POLY_ID, Standardized_Residuals)), by = "POLY_ID") %>% 
  ggplot(.) +
  geom_sf(aes(fill = Standardized_Residuals), color = "transparent") +  # Use geom_sf to map the polygons
  scale_fill_gradientn(colors = c("#FAF9F6", "#c44536"), 
                       name = "Std Residuals", 
                       na.value = "transparent") +  # Choose a color palette, invert direction if needed
  labs(title = "Choropleth Map of Standardized Residuals") +
  theme(legend.text = element_text(size = 9),
        legend.title = element_text(size = 10),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        plot.subtitle = element_text(size = 9, face = "italic"),
        plot.title = element_text(size = 12, face = "bold"),
        panel.background = element_blank(),
        panel.border = element_rect(colour = "grey", fill = NA, size = 0.8))



```

## Additional Analyses - Ziyi


Present the results of the stepwise regression and state whether all 4 predictors in the original model are kept in the final model.

```{r stepwise regression, message=FALSE, warning=FALSE}

stepwise_model <-  stepAIC(fit, direction = "both")
stepwise_model$anova

```

Present the cross-validation results â€“ that is, compare the RMSE of the original model that includes all 4 predictors with the RMSE of the model that only includes PCTVACANT and MEDHHINC as predictors.

```{r cross validation, message=FALSE, warning = FALSE}

lm <-  trainControl(method = "cv", number = 5)

cvlm_model <- train(LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV100, data=reg_data, method = "lm", trControl = lm)

print(cvlm_model)

```

```{r reduce cv model,message=FALSE, warning=FALSE}

cvlm_model_reduced = train(LNMEDHVAL ~ PCTVACANT + MEDHHINC, data = reg_data, method = "lm", trControl = lm)

print(cvlm_model_reduced)
```

# Discussion and Limitations - Emily

a)	Recap what you did in the paper and your findings. Discuss what conclusions you can draw, which variables were significant and whether that was surprising or not.


b)	Talk about the quality of the model â€“ that is, state if this is a good model overall (e.g., R2, F-ratio test), and what other predictors that we didnâ€™t include in our model might be associated with our dependent variable.
Looking at the stepwise regression results, did the final model include all 4 predictors or were some dropped? What does that tell you about the quality of the model? 
Looking at the cross-validation results, was the RMSE better for the 4 predictor model or the 2 predictor model?  


c)	If you havenâ€™t done that in the Results section, talk explicitly about the limitations of the model â€“ that is, mention which assumptions were violated, and if applicable, how that may affect the model/parameter estimation/estimated significance. 
In addition, talk about the limitations of using the NBELPOV100 variable as a predictor â€“ that is, what are some limitations of using the raw number of households living in poverty rather than a percentage?


d)	Would it make sense to run Ridge or LASSO regression here? Explain briefly (~4-5 sentences) what these methods are, when theyâ€™re used, and why they would or would not be appropriate here.


# References







